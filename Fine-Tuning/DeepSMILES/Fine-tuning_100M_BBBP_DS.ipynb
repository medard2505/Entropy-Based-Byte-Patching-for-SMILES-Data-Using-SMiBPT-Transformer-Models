{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dd142c-eea2-4a6d-903e-b4e590e0fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install rdkit-pypi\n",
    "# !pip3 install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6659a58-b95d-4a28-b4e0-5a71a5a4edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error, r2_score\n",
    "from transformers import get_scheduler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ef26ad-144f-459f-b2a1-51470ebdc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger  # Import RDKit Logger\n",
    "\n",
    "# Suppress all RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f5df08-38c3-4c90-8694-5cc162cdab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pretrained model\n",
    "import nbimporter\n",
    "from Exp_6l_8h_192d_1024ff_100M_molbert_DeepSMILES import SMILESMLMTransformer, dynamic_byte_patching, compute_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49476e59-f315-4c96-9e84-77ccde39ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b48f03b-bcd1-4456-bc8a-9258d8e0ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamDataset(Dataset):\n",
    "    def __init__(self, smiles_list, labels, max_len=128):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        patches, entropies = dynamic_byte_patching(smiles)\n",
    "\n",
    "        # Basic safeguards\n",
    "        if not patches:\n",
    "            patches = [[0]]\n",
    "            entropies = [0.0]\n",
    "\n",
    "        # # Optional truncation\n",
    "        # patches = patches[:self.max_len]\n",
    "        # entropies = entropies[:self.max_len]\n",
    "\n",
    "        return patches, entropies, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4d9fdd-5eb1-49a5-a146-d04abc40affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_finetune(batch, pad_value=0):\n",
    "#     # Sort batch by patch length (descending) â€” optional for packed sequence use\n",
    "#     batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "#     # # Debug print: shape inspection\n",
    "#     # for p, _, _ in batch:\n",
    "#     #     print(f\"[DEBUG] Patch count: {len(p)} | Patch shapes: {[len(i) if hasattr(i, '__len__') else type(i) for i in p]}\")\n",
    "\n",
    "#     # Convert patches (List[List[int]]) into padded 3D tensor [B, P, L]\n",
    "#     patch_tensors = [torch.tensor(p, dtype=torch.long) for p, _, _ in batch]\n",
    "#     max_patches = max([len(p) for p in patch_tensors])\n",
    "#     max_patch_len = max([len(subpatch) for p in patch_tensors for subpatch in p])\n",
    "    \n",
    "#     padded_patches = torch.full((len(batch), max_patches, max_patch_len), pad_value, dtype=torch.long)\n",
    "#     for i, patch_seq in enumerate(patch_tensors):\n",
    "#         for j, patch in enumerate(patch_seq):\n",
    "#             padded_patches[i, j, :len(patch)] = patch\n",
    "\n",
    "#     # Convert and pad entropy tensors to [B, P]\n",
    "#     entropy_tensors = [torch.tensor(e, dtype=torch.float32) for _, e, _ in batch]\n",
    "#     entropy_padded = pad_sequence(entropy_tensors, batch_first=True, padding_value=0.0)\n",
    "\n",
    "#     # Stack label tensors [B, ...]\n",
    "#     label_tensors = [torch.tensor(l, dtype=torch.float32) for _, _, l in batch]\n",
    "#     label_tensor = torch.stack(label_tensors)\n",
    "\n",
    "#     return padded_patches, entropy_padded, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27eccf99-8d06-4959-83d7-f9bb8960e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_finetune(batch, pad_value=0):\n",
    "    # Extract the components\n",
    "    patch_lists = [item[0] for item in batch]  # List of List[List[int]]\n",
    "    entropy_lists = [item[1] for item in batch]  # List of List[float]\n",
    "    label_list = [item[2] for item in batch]  # List of labels (float or vector)\n",
    "\n",
    "    # Determine max patch count and max patch length\n",
    "    max_num_patches = max(len(p) for p in patch_lists)\n",
    "    max_patch_len = max(len(patch) for p in patch_lists for patch in p)\n",
    "\n",
    "    # Prepare padded patch tensor: [B, P, L]\n",
    "    padded_patches = torch.full(\n",
    "        (len(batch), max_num_patches, max_patch_len), pad_value, dtype=torch.long\n",
    "    )\n",
    "    for i, patch_seq in enumerate(patch_lists):\n",
    "        for j, patch in enumerate(patch_seq):\n",
    "            padded_patches[i, j, :len(patch)] = torch.tensor(patch, dtype=torch.long)\n",
    "\n",
    "    # Pad entropy tensors to [B, P]\n",
    "    entropy_tensors = [torch.tensor(e, dtype=torch.float32) for e in entropy_lists]\n",
    "    entropy_padded = pad_sequence(entropy_tensors, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Stack labels to [B, ...]\n",
    "    label_tensor = torch.stack([torch.tensor(l, dtype=torch.float32) for l in label_list])\n",
    "\n",
    "    return padded_patches, entropy_padded, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3dce45-96e9-4dd5-8c8a-74fcbb6932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneHead(nn.Module):\n",
    "    def __init__(self, base_model, task=\"classification\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task = task\n",
    "        self.head = None  # delayed init\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)  # [B, L, D]\n",
    "        x = x.mean(dim=1)       # [B, D]\n",
    "        if self.head is None:\n",
    "            self.head = nn.Linear(x.size(-1), 1).to(x.device)  # lazy init\n",
    "        return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616c446a-6518-45ea-aee3-704a58c3c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(model, optimizer, scheduler, criterion, dataloader, device):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    for xb, _, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "\n",
    "        if criterion._get_name() in [\"BCEWithLogitsLoss\", \"MSELoss\"]:\n",
    "            loss = criterion(preds, yb.float())\n",
    "        else:\n",
    "            loss = criterion(preds, yb.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d047b65e-e2f0-4878-856c-faabebc9c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fold(model, dataloader, task, device, multi_task=False):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, _, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            outputs = model(xb).detach().cpu()\n",
    "            preds.append(outputs)\n",
    "            labels.append(yb)\n",
    "\n",
    "    y_true = torch.cat(labels).numpy()\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "\n",
    "    if multi_task:\n",
    "        aucs, f1s = [], []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            if np.sum(~np.isnan(y_true[:, i])) == 0:\n",
    "                aucs.append(np.nan)\n",
    "                f1s.append(np.nan)\n",
    "                continue\n",
    "            mask = ~np.isnan(y_true[:, i])\n",
    "            aucs.append(roc_auc_score(y_true[mask, i], y_pred[mask, i]))\n",
    "            f1s.append(f1_score(y_true[mask, i], y_pred[mask, i] > 0.5))\n",
    "        return np.array(aucs), np.array(f1s)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        if y_pred.ndim == 1 or y_pred.shape[1] == 1:\n",
    "            # Binary classification\n",
    "            y_prob = torch.sigmoid(torch.tensor(y_pred)).numpy()\n",
    "            y_pred_cls = (y_prob >= 0.5).astype(int)\n",
    "        else:\n",
    "            # Multiclass classification\n",
    "            y_prob = torch.softmax(torch.tensor(y_pred), dim=1).numpy()\n",
    "            y_pred_cls = np.argmax(y_pred, axis=1)\n",
    "            y_prob = y_prob[:, 1]  # Assuming class 1 is the positive class\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        f1 = f1_score(y_true, y_pred_cls)\n",
    "        return auc, f1\n",
    "\n",
    "    else:\n",
    "        # Regression\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801ca4a4-e507-42b1-a4ca-b1c0cf4abce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SMILESMLMTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size=256, embedding_dim=192, num_heads=8, num_layers=6, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.vocab_size = vocab_size + 2  # Expand vocab for special tokens\n",
    "#         self.embedding_dim = embedding_dim\n",
    "        \n",
    "#         self.embedding = DynamicBytePatchEmbedding(self.vocab_size, embedding_dim)\n",
    "#         self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=embedding_dim,\n",
    "#             nhead=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             dim_feedforward=1024,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "#         self.lm_head = nn.Linear(embedding_dim, self.vocab_size)  # for pretraining\n",
    "\n",
    "#     def forward(self, x, entropy=None, attention_mask=None):\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "#         if attention_mask is not None:\n",
    "#             x = self.encoder(x, attention_mask=attention_mask)\n",
    "#         else:\n",
    "#             x = self.encoder(x)\n",
    "\n",
    "#         if entropy is not None:\n",
    "#             entropy = entropy.unsqueeze(-1)\n",
    "#             x = x * (1 + entropy)  # entropy-aware reweighting\n",
    "\n",
    "#         return x  # let FinetuneHead handle classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e9470d3-ce7b-4ea5-bd51-6752e442145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converter(rings=True, branches=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DeepSMILES converter\n",
    "import deepsmiles \n",
    "\n",
    "converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "print(converter) # record the options used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb019088-cd0b-467a-8b06-c61f217fa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_deepsmiles(smiles_list):\n",
    "    encoded = []\n",
    "    failed = []\n",
    "    for smiles in smiles_list:\n",
    "        try:\n",
    "            if smiles and isinstance(smiles, str):\n",
    "                encoded_smiles = converter.encode(smiles)\n",
    "                encoded.append(encoded_smiles)\n",
    "            else:\n",
    "                failed.append(smiles)\n",
    "        except Exception:\n",
    "            failed.append(smiles)\n",
    "    return encoded, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a375d6e5-4633-4960-912a-c6ea68276883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finetuning(task_name=\"BBBP\", path=\"best_model_100M_DeepSMILES.pth\", num_folds=10, batch_size=64, epochs=10, dataset_path=\"path_to_your_dataset.csv\"):\n",
    "    # Load the dataset directly from CSV\n",
    "    dataset = pd.read_csv(dataset_path, encoding='ISO-8859-1')\n",
    "    dataset.head()\n",
    "    \n",
    "    # Extract SMILES and labels (assuming the dataset has columns 'smiles' and 'label')\n",
    "    smiles_samples = dataset['smiles'].tolist()\n",
    "    # Convert SMILES samples to DeepSMILES\n",
    "    deepsmiles_samples, failed_smiles = smiles_to_deepsmiles(smiles_samples)\n",
    "    print(f\"Successfully encoded {len(deepsmiles_samples)} SMILES, failed {len(failed_smiles)}.\")\n",
    "    smiles = deepsmiles_samples\n",
    "    labels = dataset['label'].tolist()\n",
    "\n",
    "    # Define task type (classification or regression)\n",
    "    if task_name in {\"BBBP\", \"Tox21\"}:\n",
    "        task_type = \"classification\"\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "\n",
    "    # Handle multi-task classification (e.g., Tox21)\n",
    "    multi_task = True if task_name == \"Tox21\" else False\n",
    "\n",
    "    # Set up K-Fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(smiles)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{num_folds} ---\")\n",
    "\n",
    "        # Initialize the base model and load the pretrained weights\n",
    "        base_model = SMILESMLMTransformer()\n",
    "        state_dict = torch.load(path, weights_only=True)\n",
    "        base_model.load_state_dict(state_dict)\n",
    "        # base_model.load_state_dict(torch.load(path))\n",
    "        model = FinetuneHead(base_model, task=task_type).to(device)\n",
    "\n",
    "        # Get train and test labels\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        test_labels = [labels[i] for i in test_idx]\n",
    "\n",
    "        # Prepare datasets and dataloaders\n",
    "        train_dataset = DownstreamDataset([smiles[i] for i in train_idx], train_labels)\n",
    "        test_dataset = DownstreamDataset([smiles[i] for i in test_idx], test_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_finetune)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_finetune)\n",
    "\n",
    "        # Optimizer, scheduler, and loss function\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "        scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "        criterion = nn.BCEWithLogitsLoss() if task_type == \"classification\" else nn.MSELoss()\n",
    "\n",
    "        # Training and evaluation loop\n",
    "        for epoch in range(epochs):\n",
    "            loss = train_fold(model, optimizer, scheduler, criterion, train_loader, device)\n",
    "            metrics = evaluate_fold(model, test_loader, task_type, device, multi_task=multi_task)\n",
    "            print(f\"Epoch {epoch+1:02d} | Loss: {loss:.4f} | Metrics: {metrics}\")\n",
    "\n",
    "        all_scores.append(metrics)\n",
    "\n",
    "    # Final results\n",
    "    print(f\"\\nFinal {task_name} {task_type.upper()} Results over {num_folds} folds:\")\n",
    "    scores_np = np.array(all_scores)\n",
    "    if multi_task:\n",
    "        auc_mean = np.nanmean(scores_np[:, 0])\n",
    "        f1_mean = np.nanmean(scores_np[:, 1])\n",
    "        print(f\"Mean AUC (macro): {auc_mean:.4f}\")\n",
    "        print(f\"Mean F1  (macro): {f1_mean:.4f}\")\n",
    "    elif task_type == \"classification\":\n",
    "        print(f\"AUC: {scores_np[:, 0].mean():.4f} Â± {scores_np[:, 0].std():.4f}\")\n",
    "        print(f\"F1 : {scores_np[:, 1].mean():.4f} Â± {scores_np[:, 1].std():.4f}\")\n",
    "    else:\n",
    "        print(f\"RMSE: {scores_np[:, 0].mean():.4f} Â± {scores_np[:, 0].std():.4f}\")\n",
    "        print(f\"RÂ²  : {scores_np[:, 1].mean():.4f} Â± {scores_np[:, 1].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe53bcf-8e23-43c6-a912-7128c9d71a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully encoded 2050 SMILES, failed 0.\n",
      "\n",
      "--- Fold 1/10 ---\n",
      "Epoch 01 | Loss: 1.7327 | Metrics: (0.7070063694267517, 0.8674033149171271)\n",
      "Epoch 02 | Loss: 0.5033 | Metrics: (0.8527070063694269, 0.8402777777777778)\n",
      "Epoch 03 | Loss: 0.3242 | Metrics: (0.916135881104034, 0.9)\n",
      "Epoch 04 | Loss: 0.2303 | Metrics: (0.9320594479830149, 0.9066666666666667)\n",
      "Epoch 05 | Loss: 0.2151 | Metrics: (0.9339171974522293, 0.9)\n",
      "Epoch 06 | Loss: 0.1774 | Metrics: (0.9365711252653928, 0.9049180327868853)\n",
      "Epoch 07 | Loss: 0.1556 | Metrics: (0.9359076433121019, 0.8949152542372882)\n",
      "Epoch 08 | Loss: 0.1474 | Metrics: (0.9388269639065818, 0.9108910891089109)\n",
      "Epoch 09 | Loss: 0.1234 | Metrics: (0.937367303609342, 0.9120521172638437)\n",
      "Epoch 10 | Loss: 0.1175 | Metrics: (0.9373673036093418, 0.9072847682119206)\n",
      "\n",
      "--- Fold 2/10 ---\n",
      "Epoch 01 | Loss: 1.7389 | Metrics: (0.49192028009695665, 0.8705234159779615)\n",
      "Epoch 02 | Loss: 0.5246 | Metrics: (0.7790196606517641, 0.8705234159779615)\n",
      "Epoch 03 | Loss: 0.3658 | Metrics: (0.9127390250471317, 0.9108910891089109)\n",
      "Epoch 04 | Loss: 0.2556 | Metrics: (0.9208187449501751, 0.9196141479099678)\n",
      "Epoch 05 | Loss: 0.2152 | Metrics: (0.9239159709130084, 0.8956228956228958)\n",
      "Epoch 06 | Loss: 0.1898 | Metrics: (0.9247912739025047, 0.891156462585034)\n",
      "Epoch 07 | Loss: 0.1499 | Metrics: (0.9217613789388635, 0.9215686274509803)\n",
      "Epoch 08 | Loss: 0.1402 | Metrics: (0.9189334769727981, 0.910299003322259)\n",
      "Epoch 09 | Loss: 0.1315 | Metrics: (0.9212227309453273, 0.9215686274509803)\n",
      "Epoch 10 | Loss: 0.1182 | Metrics: (0.9183948289792621, 0.9066666666666667)\n",
      "\n",
      "--- Fold 3/10 ---\n",
      "Epoch 01 | Loss: 1.1220 | Metrics: (0.7953745600804424, 0.8547486033519553)\n",
      "Epoch 02 | Loss: 0.3866 | Metrics: (0.9279788838612368, 0.9114754098360655)\n",
      "Epoch 03 | Loss: 0.2601 | Metrics: (0.9409250879839115, 0.8802816901408452)\n",
      "Epoch 04 | Loss: 0.2306 | Metrics: (0.9502262443438914, 0.9363057324840764)\n",
      "Epoch 05 | Loss: 0.1865 | Metrics: (0.9463298139768728, 0.8873239436619719)\n",
      "Epoch 06 | Loss: 0.1705 | Metrics: (0.9467068878833585, 0.9285714285714285)\n",
      "Epoch 07 | Loss: 0.1563 | Metrics: (0.9448215183509301, 0.922077922077922)\n",
      "Epoch 08 | Loss: 0.1456 | Metrics: (0.9479638009049773, 0.9285714285714285)\n",
      "Epoch 09 | Loss: 0.1226 | Metrics: (0.9475867269984917, 0.9285714285714285)\n",
      "Epoch 10 | Loss: 0.1163 | Metrics: (0.9487179487179487, 0.9285714285714285)\n",
      "\n",
      "--- Fold 4/10 ---\n",
      "Epoch 01 | Loss: 6.8603 | Metrics: (0.5153927813163481, 0.8674033149171271)\n",
      "Epoch 02 | Loss: 0.5790 | Metrics: (0.7070063694267515, 0.8674033149171271)\n",
      "Epoch 03 | Loss: 0.5352 | Metrics: (0.7143046709129511, 0.8674033149171271)\n",
      "Epoch 04 | Loss: 0.5057 | Metrics: (0.8032112526539278, 0.8698060941828256)\n",
      "Epoch 05 | Loss: 0.4012 | Metrics: (0.9117569002123141, 0.8786127167630058)\n",
      "Epoch 06 | Loss: 0.3249 | Metrics: (0.9305997876857749, 0.9177215189873418)\n",
      "Epoch 07 | Loss: 0.2649 | Metrics: (0.9328556263269638, 0.914826498422713)\n",
      "Epoch 08 | Loss: 0.2469 | Metrics: (0.9414808917197451, 0.9158878504672897)\n",
      "Epoch 09 | Loss: 0.2274 | Metrics: (0.9458598726114651, 0.9245283018867924)\n",
      "Epoch 10 | Loss: 0.2274 | Metrics: (0.9478503184713377, 0.9320987654320988)\n",
      "\n",
      "--- Fold 5/10 ---\n",
      "Epoch 01 | Loss: 0.6879 | Metrics: (0.6766060606060607, 0.8249258160237388)\n",
      "Epoch 02 | Loss: 0.3556 | Metrics: (0.9233939393939393, 0.8958333333333334)\n",
      "Epoch 03 | Loss: 0.2519 | Metrics: (0.9453333333333334, 0.9163879598662208)\n",
      "Epoch 04 | Loss: 0.1943 | Metrics: (0.9430303030303029, 0.9066666666666666)\n",
      "Epoch 05 | Loss: 0.1603 | Metrics: (0.9461818181818181, 0.9194630872483222)\n",
      "Epoch 06 | Loss: 0.1338 | Metrics: (0.9476363636363637, 0.9169435215946843)\n",
      "Epoch 07 | Loss: 0.1150 | Metrics: (0.9416969696969696, 0.9121621621621622)\n",
      "Epoch 08 | Loss: 0.0995 | Metrics: (0.9437575757575759, 0.9163879598662208)\n",
      "Epoch 09 | Loss: 0.0846 | Metrics: (0.9375757575757576, 0.9090909090909091)\n",
      "Epoch 10 | Loss: 0.0893 | Metrics: (0.9402424242424243, 0.912751677852349)\n",
      "\n",
      "--- Fold 6/10 ---\n",
      "Epoch 01 | Loss: 3.3689 | Metrics: (0.47320207820617993, 0.8736263736263736)\n",
      "Epoch 02 | Loss: 0.5493 | Metrics: (0.6326223680612524, 0.8736263736263736)\n",
      "Epoch 03 | Loss: 0.5097 | Metrics: (0.7670221493027072, 0.8736263736263736)\n",
      "Epoch 04 | Loss: 0.3640 | Metrics: (0.9161881323489199, 0.8903654485049834)\n",
      "Epoch 05 | Loss: 0.2856 | Metrics: (0.938200710965272, 0.9166666666666666)\n",
      "Epoch 06 | Loss: 0.2340 | Metrics: (0.9483182936833471, 0.9345794392523364)\n",
      "Epoch 07 | Loss: 0.2112 | Metrics: (0.9529669127700301, 0.9240924092409241)\n",
      "Epoch 08 | Loss: 0.1959 | Metrics: (0.9543341536778781, 0.9419354838709677)\n",
      "Epoch 09 | Loss: 0.1805 | Metrics: (0.9529669127700301, 0.9426751592356688)\n",
      "Epoch 10 | Loss: 0.1804 | Metrics: (0.9547443259502325, 0.9423076923076923)\n",
      "\n",
      "--- Fold 7/10 ---\n",
      "Epoch 01 | Loss: 4.8266 | Metrics: (0.5846602972399151, 0.8674033149171271)\n",
      "Epoch 02 | Loss: 0.5613 | Metrics: (0.708731422505308, 0.8674033149171271)\n",
      "Epoch 03 | Loss: 0.5193 | Metrics: (0.807988322717622, 0.8674033149171271)\n",
      "Epoch 04 | Loss: 0.4080 | Metrics: (0.9233014861995754, 0.913946587537092)\n",
      "Epoch 05 | Loss: 0.3080 | Metrics: (0.9530254777070064, 0.9333333333333333)\n",
      "Epoch 06 | Loss: 0.2723 | Metrics: (0.9558121019108281, 0.94375)\n",
      "Epoch 07 | Loss: 0.2374 | Metrics: (0.9567409766454352, 0.9353846153846153)\n",
      "Epoch 08 | Loss: 0.2194 | Metrics: (0.9612526539278132, 0.9299363057324841)\n",
      "Epoch 09 | Loss: 0.2082 | Metrics: (0.9612526539278132, 0.9333333333333332)\n",
      "Epoch 10 | Loss: 0.1977 | Metrics: (0.9609872611464968, 0.94375)\n",
      "\n",
      "--- Fold 8/10 ---\n",
      "Epoch 01 | Loss: 1.0549 | Metrics: (0.8309451985922574, 0.8547486033519553)\n",
      "Epoch 02 | Loss: 0.3637 | Metrics: (0.9524886877828054, 0.895104895104895)\n",
      "Epoch 03 | Loss: 0.2493 | Metrics: (0.9548768225238813, 0.9152542372881357)\n",
      "Epoch 04 | Loss: 0.2116 | Metrics: (0.9718451483157364, 0.9364548494983277)\n",
      "Epoch 05 | Loss: 0.1951 | Metrics: (0.9722222222222222, 0.9368770764119602)\n",
      "Epoch 06 | Loss: 0.1589 | Metrics: (0.9673831070889894, 0.9266666666666667)\n",
      "Epoch 07 | Loss: 0.1301 | Metrics: (0.966691804927099, 0.9355932203389831)\n",
      "Epoch 08 | Loss: 0.1240 | Metrics: (0.9695827048768225, 0.9355932203389831)\n",
      "Epoch 09 | Loss: 0.1059 | Metrics: (0.9671945701357466, 0.9209621993127148)\n",
      "Epoch 10 | Loss: 0.1030 | Metrics: (0.9659376571141277, 0.9251700680272108)\n",
      "\n",
      "--- Fold 9/10 ---\n",
      "Epoch 01 | Loss: 1.3073 | Metrics: (0.7353603603603603, 0.900804289544236)\n",
      "Epoch 02 | Loss: 0.4846 | Metrics: (0.9041184041184042, 0.8646864686468647)\n",
      "Epoch 03 | Loss: 0.3054 | Metrics: (0.9631595881595881, 0.9467455621301775)\n",
      "Epoch 04 | Loss: 0.2461 | Metrics: (0.9707207207207207, 0.9454545454545454)\n",
      "Epoch 05 | Loss: 0.1956 | Metrics: (0.9747425997425997, 0.9473684210526314)\n",
      "Epoch 06 | Loss: 0.1711 | Metrics: (0.9763513513513514, 0.951219512195122)\n",
      "Epoch 07 | Loss: 0.1615 | Metrics: (0.9761904761904762, 0.9408099688473521)\n",
      "Epoch 08 | Loss: 0.1472 | Metrics: (0.976029601029601, 0.9580838323353292)\n",
      "Epoch 09 | Loss: 0.1241 | Metrics: (0.9752252252252253, 0.9580838323353292)\n",
      "Epoch 10 | Loss: 0.1118 | Metrics: (0.9758687258687259, 0.9580838323353292)\n",
      "\n",
      "--- Fold 10/10 ---\n",
      "Epoch 01 | Loss: 2.4141 | Metrics: (0.6303225806451612, 0.8611111111111112)\n",
      "Epoch 02 | Loss: 0.5471 | Metrics: (0.7771612903225806, 0.8611111111111112)\n",
      "Epoch 03 | Loss: 0.4880 | Metrics: (0.9254193548387097, 0.913946587537092)\n",
      "Epoch 04 | Loss: 0.3546 | Metrics: (0.9676129032258063, 0.9194029850746268)\n",
      "Epoch 05 | Loss: 0.2881 | Metrics: (0.966967741935484, 0.9433962264150944)\n",
      "Epoch 06 | Loss: 0.2544 | Metrics: (0.9769032258064516, 0.9612903225806452)\n",
      "Epoch 07 | Loss: 0.2190 | Metrics: (0.9780645161290323, 0.9523809523809523)\n",
      "Epoch 08 | Loss: 0.2030 | Metrics: (0.975741935483871, 0.9477124183006537)\n",
      "Epoch 09 | Loss: 0.1843 | Metrics: (0.9762580645161291, 0.9483870967741934)\n",
      "Epoch 10 | Loss: 0.1829 | Metrics: (0.975225806451613, 0.9517684887459806)\n",
      "\n",
      "Final BBBP CLASSIFICATION Results over 10 folds:\n",
      "AUC: 0.9525 Â± 0.0170\n",
      "F1 : 0.9308 Â± 0.0173\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_finetuning(\"BBBP\", dataset_path=\"BBBP.csv\")  # Or \"ESOL\", \"Tox21\", \"Lipophilicity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cb964-bb76-453f-867c-a3a72f2ddcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
