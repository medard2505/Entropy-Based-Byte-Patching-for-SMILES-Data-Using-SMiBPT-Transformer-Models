{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dd142c-eea2-4a6d-903e-b4e590e0fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install rdkit-pypi\n",
    "# !pip3 install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6659a58-b95d-4a28-b4e0-5a71a5a4edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error, r2_score\n",
    "from transformers import get_scheduler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ef26ad-144f-459f-b2a1-51470ebdc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger  # Import RDKit Logger\n",
    "\n",
    "# Suppress all RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f5df08-38c3-4c90-8694-5cc162cdab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pretrained model\n",
    "import nbimporter\n",
    "from Exp_6l_8h_192d_1024ff_100M_molbert_DeepSMILES import SMILESMLMTransformer, dynamic_byte_patching, compute_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49476e59-f315-4c96-9e84-77ccde39ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b48f03b-bcd1-4456-bc8a-9258d8e0ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamDataset(Dataset):\n",
    "    def __init__(self, smiles_list, labels, max_len=128):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        patches, entropies = dynamic_byte_patching(smiles)\n",
    "\n",
    "        # Basic safeguards\n",
    "        if not patches:\n",
    "            patches = [[0]]\n",
    "            entropies = [0.0]\n",
    "\n",
    "        # # Optional truncation\n",
    "        # patches = patches[:self.max_len]\n",
    "        # entropies = entropies[:self.max_len]\n",
    "\n",
    "        return patches, entropies, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4d9fdd-5eb1-49a5-a146-d04abc40affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_finetune(batch, pad_value=0):\n",
    "#     # Sort batch by patch length (descending) — optional for packed sequence use\n",
    "#     batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "#     # # Debug print: shape inspection\n",
    "#     # for p, _, _ in batch:\n",
    "#     #     print(f\"[DEBUG] Patch count: {len(p)} | Patch shapes: {[len(i) if hasattr(i, '__len__') else type(i) for i in p]}\")\n",
    "\n",
    "#     # Convert patches (List[List[int]]) into padded 3D tensor [B, P, L]\n",
    "#     patch_tensors = [torch.tensor(p, dtype=torch.long) for p, _, _ in batch]\n",
    "#     max_patches = max([len(p) for p in patch_tensors])\n",
    "#     max_patch_len = max([len(subpatch) for p in patch_tensors for subpatch in p])\n",
    "    \n",
    "#     padded_patches = torch.full((len(batch), max_patches, max_patch_len), pad_value, dtype=torch.long)\n",
    "#     for i, patch_seq in enumerate(patch_tensors):\n",
    "#         for j, patch in enumerate(patch_seq):\n",
    "#             padded_patches[i, j, :len(patch)] = patch\n",
    "\n",
    "#     # Convert and pad entropy tensors to [B, P]\n",
    "#     entropy_tensors = [torch.tensor(e, dtype=torch.float32) for _, e, _ in batch]\n",
    "#     entropy_padded = pad_sequence(entropy_tensors, batch_first=True, padding_value=0.0)\n",
    "\n",
    "#     # Stack label tensors [B, ...]\n",
    "#     label_tensors = [torch.tensor(l, dtype=torch.float32) for _, _, l in batch]\n",
    "#     label_tensor = torch.stack(label_tensors)\n",
    "\n",
    "#     return padded_patches, entropy_padded, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27eccf99-8d06-4959-83d7-f9bb8960e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_finetune(batch, pad_value=0):\n",
    "    # Extract the components\n",
    "    patch_lists = [item[0] for item in batch]  # List of List[List[int]]\n",
    "    entropy_lists = [item[1] for item in batch]  # List of List[float]\n",
    "    label_list = [item[2] for item in batch]  # List of labels (float or vector)\n",
    "\n",
    "    # Determine max patch count and max patch length\n",
    "    max_num_patches = max(len(p) for p in patch_lists)\n",
    "    max_patch_len = max(len(patch) for p in patch_lists for patch in p)\n",
    "\n",
    "    # Prepare padded patch tensor: [B, P, L]\n",
    "    padded_patches = torch.full(\n",
    "        (len(batch), max_num_patches, max_patch_len), pad_value, dtype=torch.long\n",
    "    )\n",
    "    for i, patch_seq in enumerate(patch_lists):\n",
    "        for j, patch in enumerate(patch_seq):\n",
    "            padded_patches[i, j, :len(patch)] = torch.tensor(patch, dtype=torch.long)\n",
    "\n",
    "    # Pad entropy tensors to [B, P]\n",
    "    entropy_tensors = [torch.tensor(e, dtype=torch.float32) for e in entropy_lists]\n",
    "    entropy_padded = pad_sequence(entropy_tensors, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Stack labels to [B, ...]\n",
    "    label_tensor = torch.stack([torch.tensor(l, dtype=torch.float32) for l in label_list])\n",
    "\n",
    "    return padded_patches, entropy_padded, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3dce45-96e9-4dd5-8c8a-74fcbb6932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneHead(nn.Module):\n",
    "    def __init__(self, base_model, task=\"classification\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task = task\n",
    "        self.head = None  # delayed init\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)  # [B, L, D]\n",
    "        x = x.mean(dim=1)       # [B, D]\n",
    "        if self.head is None:\n",
    "            self.head = nn.Linear(x.size(-1), 1).to(x.device)  # lazy init\n",
    "        return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616c446a-6518-45ea-aee3-704a58c3c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(model, optimizer, scheduler, criterion, dataloader, device):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    for xb, _, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "\n",
    "        if criterion._get_name() in [\"BCEWithLogitsLoss\", \"MSELoss\"]:\n",
    "            loss = criterion(preds, yb.float())\n",
    "        else:\n",
    "            loss = criterion(preds, yb.long())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d047b65e-e2f0-4878-856c-faabebc9c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fold(model, dataloader, task, device, multi_task=False):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, _, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            outputs = model(xb).detach().cpu()\n",
    "            preds.append(outputs)\n",
    "            labels.append(yb)\n",
    "\n",
    "    y_true = torch.cat(labels).numpy()\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "\n",
    "    if multi_task:\n",
    "        aucs, f1s = [], []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            if np.sum(~np.isnan(y_true[:, i])) == 0:\n",
    "                aucs.append(np.nan)\n",
    "                f1s.append(np.nan)\n",
    "                continue\n",
    "            mask = ~np.isnan(y_true[:, i])\n",
    "            aucs.append(roc_auc_score(y_true[mask, i], y_pred[mask, i]))\n",
    "            f1s.append(f1_score(y_true[mask, i], y_pred[mask, i] > 0.5))\n",
    "        return np.array(aucs), np.array(f1s)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        if y_pred.ndim == 1 or y_pred.shape[1] == 1:\n",
    "            # Binary classification\n",
    "            y_prob = torch.sigmoid(torch.tensor(y_pred)).numpy()\n",
    "            y_pred_cls = (y_prob >= 0.5).astype(int)\n",
    "        else:\n",
    "            # Multiclass classification\n",
    "            y_prob = torch.softmax(torch.tensor(y_pred), dim=1).numpy()\n",
    "            y_pred_cls = np.argmax(y_pred, axis=1)\n",
    "            y_prob = y_prob[:, 1]  # Assuming class 1 is the positive class\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        f1 = f1_score(y_true, y_pred_cls)\n",
    "        return auc, f1\n",
    "\n",
    "    else:\n",
    "        # Regression\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801ca4a4-e507-42b1-a4ca-b1c0cf4abce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SMILESMLMTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size=256, embedding_dim=192, num_heads=8, num_layers=6, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.vocab_size = vocab_size + 2  # Expand vocab for special tokens\n",
    "#         self.embedding_dim = embedding_dim\n",
    "        \n",
    "#         self.embedding = DynamicBytePatchEmbedding(self.vocab_size, embedding_dim)\n",
    "#         self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=embedding_dim,\n",
    "#             nhead=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             dim_feedforward=1024,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "#         self.lm_head = nn.Linear(embedding_dim, self.vocab_size)  # for pretraining\n",
    "\n",
    "#     def forward(self, x, entropy=None, attention_mask=None):\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "#         if attention_mask is not None:\n",
    "#             x = self.encoder(x, attention_mask=attention_mask)\n",
    "#         else:\n",
    "#             x = self.encoder(x)\n",
    "\n",
    "#         if entropy is not None:\n",
    "#             entropy = entropy.unsqueeze(-1)\n",
    "#             x = x * (1 + entropy)  # entropy-aware reweighting\n",
    "\n",
    "#         return x  # let FinetuneHead handle classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e9470d3-ce7b-4ea5-bd51-6752e442145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converter(rings=True, branches=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DeepSMILES converter\n",
    "import deepsmiles \n",
    "\n",
    "converter = deepsmiles.Converter(rings=True, branches=True)\n",
    "print(converter) # record the options used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb019088-cd0b-467a-8b06-c61f217fa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_deepsmiles(smiles_list):\n",
    "    encoded = []\n",
    "    failed = []\n",
    "    for smiles in smiles_list:\n",
    "        try:\n",
    "            if smiles and isinstance(smiles, str):\n",
    "                encoded_smiles = converter.encode(smiles)\n",
    "                encoded.append(encoded_smiles)\n",
    "            else:\n",
    "                failed.append(smiles)\n",
    "        except Exception:\n",
    "            failed.append(smiles)\n",
    "    return encoded, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a375d6e5-4633-4960-912a-c6ea68276883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finetuning(task_name=\"BBBP\", path=\"best_model_100M_DeepSMILES.pth\", num_folds=10, batch_size=64, epochs=10, dataset_path=\"path_to_your_dataset.csv\"):\n",
    "    # Load the dataset directly from CSV\n",
    "    dataset = pd.read_csv(dataset_path, encoding='ISO-8859-1')\n",
    "    dataset.head()\n",
    "    \n",
    "    # Extract SMILES and labels (assuming the dataset has columns 'smiles' and 'label')\n",
    "    smiles_samples = dataset['smiles'].tolist()\n",
    "    # Convert SMILES samples to DeepSMILES\n",
    "    deepsmiles_samples, failed_smiles = smiles_to_deepsmiles(smiles_samples)\n",
    "    print(f\"Successfully encoded {len(deepsmiles_samples)} SMILES, failed {len(failed_smiles)}.\")\n",
    "    smiles = deepsmiles_samples\n",
    "    labels = dataset['label'].tolist()\n",
    "\n",
    "    # Define task type (classification or regression)\n",
    "    if task_name in {\"BBBP\", \"Tox21\"}:\n",
    "        task_type = \"classification\"\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "\n",
    "    # Handle multi-task classification (e.g., Tox21)\n",
    "    multi_task = True if task_name == \"Tox21\" else False\n",
    "\n",
    "    # Set up K-Fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(smiles)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{num_folds} ---\")\n",
    "\n",
    "        # Initialize the base model and load the pretrained weights\n",
    "        base_model = SMILESMLMTransformer()\n",
    "        state_dict = torch.load(path, weights_only=True)\n",
    "        base_model.load_state_dict(state_dict)\n",
    "        # base_model.load_state_dict(torch.load(path))\n",
    "        model = FinetuneHead(base_model, task=task_type).to(device)\n",
    "\n",
    "        # Get train and test labels\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        test_labels = [labels[i] for i in test_idx]\n",
    "\n",
    "        # Prepare datasets and dataloaders\n",
    "        train_dataset = DownstreamDataset([smiles[i] for i in train_idx], train_labels)\n",
    "        test_dataset = DownstreamDataset([smiles[i] for i in test_idx], test_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_finetune)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_finetune)\n",
    "\n",
    "        # Optimizer, scheduler, and loss function\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "        scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "        criterion = nn.BCEWithLogitsLoss() if task_type == \"classification\" else nn.MSELoss()\n",
    "\n",
    "        # Training and evaluation loop\n",
    "        for epoch in range(epochs):\n",
    "            loss = train_fold(model, optimizer, scheduler, criterion, train_loader, device)\n",
    "            metrics = evaluate_fold(model, test_loader, task_type, device, multi_task=multi_task)\n",
    "            print(f\"Epoch {epoch+1:02d} | Loss: {loss:.4f} | Metrics: {metrics}\")\n",
    "\n",
    "        all_scores.append(metrics)\n",
    "\n",
    "    # Final results\n",
    "    print(f\"\\nFinal {task_name} {task_type.upper()} Results over {num_folds} folds:\")\n",
    "    scores_np = np.array(all_scores)\n",
    "    if multi_task:\n",
    "        auc_mean = np.nanmean(scores_np[:, 0])\n",
    "        f1_mean = np.nanmean(scores_np[:, 1])\n",
    "        print(f\"Mean AUC (macro): {auc_mean:.4f}\")\n",
    "        print(f\"Mean F1  (macro): {f1_mean:.4f}\")\n",
    "    elif task_type == \"classification\":\n",
    "        print(f\"AUC: {scores_np[:, 0].mean():.4f} ± {scores_np[:, 0].std():.4f}\")\n",
    "        print(f\"F1 : {scores_np[:, 1].mean():.4f} ± {scores_np[:, 1].std():.4f}\")\n",
    "    else:\n",
    "        print(f\"RMSE: {scores_np[:, 0].mean():.4f} ± {scores_np[:, 0].std():.4f}\")\n",
    "        print(f\"R²  : {scores_np[:, 1].mean():.4f} ± {scores_np[:, 1].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe53bcf-8e23-43c6-a912-7128c9d71a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully encoded 4794 SMILES, failed 0.\n",
      "\n",
      "--- Fold 1/10 ---\n",
      "Epoch 01 | Loss: 1.0151 | Metrics: (0.6664585781433607, 0.0)\n",
      "Epoch 02 | Loss: 0.6639 | Metrics: (0.7138660399529965, 0.5136986301369862)\n",
      "Epoch 03 | Loss: 0.5955 | Metrics: (0.7413337250293772, 0.6241457858769932)\n",
      "Epoch 04 | Loss: 0.5345 | Metrics: (0.7760906286721503, 0.6468842729970326)\n",
      "Epoch 05 | Loss: 0.5005 | Metrics: (0.7855464159811986, 0.6723646723646723)\n",
      "Epoch 06 | Loss: 0.4656 | Metrics: (0.7995556698002351, 0.637223974763407)\n",
      "Epoch 07 | Loss: 0.4417 | Metrics: (0.8027137191539364, 0.6782608695652174)\n",
      "Epoch 08 | Loss: 0.4284 | Metrics: (0.8098193301997649, 0.6864864864864865)\n",
      "Epoch 09 | Loss: 0.4060 | Metrics: (0.8091032608695653, 0.688)\n",
      "Epoch 10 | Loss: 0.3903 | Metrics: (0.8109760575793185, 0.7016574585635359)\n",
      "\n",
      "--- Fold 2/10 ---\n",
      "Epoch 01 | Loss: 1.4231 | Metrics: (0.6235103208808083, 0.0)\n",
      "Epoch 02 | Loss: 0.6776 | Metrics: (0.6839350212001187, 0.0)\n",
      "Epoch 03 | Loss: 0.6307 | Metrics: (0.7638847690670203, 0.49068322981366463)\n",
      "Epoch 04 | Loss: 0.5843 | Metrics: (0.796356720350369, 0.6924939467312349)\n",
      "Epoch 05 | Loss: 0.5390 | Metrics: (0.8072970284936574, 0.6428571428571428)\n",
      "Epoch 06 | Loss: 0.5063 | Metrics: (0.8087278183943747, 0.6506666666666667)\n",
      "Epoch 07 | Loss: 0.4732 | Metrics: (0.8199124077402244, 0.7277108433734939)\n",
      "Epoch 08 | Loss: 0.4555 | Metrics: (0.8223203224511874, 0.6958762886597939)\n",
      "Epoch 09 | Loss: 0.4402 | Metrics: (0.8246235452181955, 0.7106598984771574)\n",
      "Epoch 10 | Loss: 0.4366 | Metrics: (0.8268744220132261, 0.7245657568238214)\n",
      "\n",
      "--- Fold 3/10 ---\n",
      "Epoch 01 | Loss: 1.0167 | Metrics: (0.7266008974998219, 0.0)\n",
      "Epoch 02 | Loss: 0.6569 | Metrics: (0.7625721205214047, 0.5284280936454848)\n",
      "Epoch 03 | Loss: 0.5859 | Metrics: (0.8120770710164542, 0.7167919799498746)\n",
      "Epoch 04 | Loss: 0.5373 | Metrics: (0.8178467127288268, 0.6449704142011835)\n",
      "Epoch 05 | Loss: 0.4982 | Metrics: (0.8191288553315763, 0.7281553398058253)\n",
      "Epoch 06 | Loss: 0.4693 | Metrics: (0.8283709665930622, 0.7272727272727273)\n",
      "Epoch 07 | Loss: 0.4279 | Metrics: (0.8367939311916803, 0.7200000000000001)\n",
      "Epoch 08 | Loss: 0.4030 | Metrics: (0.8334639219317614, 0.7209876543209877)\n",
      "Epoch 09 | Loss: 0.3903 | Metrics: (0.8392157561079849, 0.7272727272727273)\n",
      "Epoch 10 | Loss: 0.3715 | Metrics: (0.8378089607521904, 0.7277227722772277)\n",
      "\n",
      "--- Fold 4/10 ---\n",
      "Epoch 01 | Loss: 1.0769 | Metrics: (0.7459074480901424, 0.5325077399380804)\n",
      "Epoch 02 | Loss: 0.6328 | Metrics: (0.7746261781588831, 0.5939393939393939)\n",
      "Epoch 03 | Loss: 0.5812 | Metrics: (0.7854687832187656, 0.6738197424892703)\n",
      "Epoch 04 | Loss: 0.5420 | Metrics: (0.7927326199418894, 0.67012987012987)\n",
      "Epoch 05 | Loss: 0.4892 | Metrics: (0.8070476932889236, 0.670299727520436)\n",
      "Epoch 06 | Loss: 0.4620 | Metrics: (0.8123804124441926, 0.7079207920792079)\n",
      "Epoch 07 | Loss: 0.4286 | Metrics: (0.8091914109559917, 0.7142857142857143)\n",
      "Epoch 08 | Loss: 0.4103 | Metrics: (0.8185280986464462, 0.7091836734693877)\n",
      "Epoch 09 | Loss: 0.3924 | Metrics: (0.8179788817234781, 0.7177033492822966)\n",
      "Epoch 10 | Loss: 0.3803 | Metrics: (0.8214513500106302, 0.7150000000000001)\n",
      "\n",
      "--- Fold 5/10 ---\n",
      "Epoch 01 | Loss: 0.6767 | Metrics: (0.7452198682570768, 0.5602409638554218)\n",
      "Epoch 02 | Loss: 0.5760 | Metrics: (0.7743813423535695, 0.6537396121883655)\n",
      "Epoch 03 | Loss: 0.5163 | Metrics: (0.7816984155243012, 0.6818181818181819)\n",
      "Epoch 04 | Loss: 0.4801 | Metrics: (0.8084386683282891, 0.670360110803324)\n",
      "Epoch 05 | Loss: 0.4418 | Metrics: (0.8270607085632902, 0.7128205128205128)\n",
      "Epoch 06 | Loss: 0.4101 | Metrics: (0.8381876446501692, 0.7399103139013452)\n",
      "Epoch 07 | Loss: 0.3769 | Metrics: (0.8478725298201887, 0.7236180904522613)\n",
      "Epoch 08 | Loss: 0.3573 | Metrics: (0.8530354281645007, 0.7447306791569087)\n",
      "Epoch 09 | Loss: 0.3284 | Metrics: (0.8546377069610113, 0.751219512195122)\n",
      "Epoch 10 | Loss: 0.3147 | Metrics: (0.8549759658180522, 0.7381546134663342)\n",
      "\n",
      "--- Fold 6/10 ---\n",
      "Epoch 01 | Loss: 1.3180 | Metrics: (0.49151163613213555, 0.0)\n",
      "Epoch 02 | Loss: 0.6823 | Metrics: (0.5810992431208885, 0.009433962264150945)\n",
      "Epoch 03 | Loss: 0.6363 | Metrics: (0.6644089976656999, 0.43174603174603177)\n",
      "Epoch 04 | Loss: 0.5848 | Metrics: (0.7466930749098111, 0.6394557823129251)\n",
      "Epoch 05 | Loss: 0.5410 | Metrics: (0.7929900261724552, 0.6270270270270271)\n",
      "Epoch 06 | Loss: 0.4958 | Metrics: (0.8204003678291009, 0.7219512195121951)\n",
      "Epoch 07 | Loss: 0.4681 | Metrics: (0.8384204569569216, 0.6951871657754011)\n",
      "Epoch 08 | Loss: 0.4386 | Metrics: (0.8348129023130791, 0.733644859813084)\n",
      "Epoch 09 | Loss: 0.4260 | Metrics: (0.8518073141401995, 0.7282321899736146)\n",
      "Epoch 10 | Loss: 0.4080 | Metrics: (0.8513652118554149, 0.736842105263158)\n",
      "\n",
      "--- Fold 7/10 ---\n",
      "Epoch 01 | Loss: 1.2866 | Metrics: (0.7011652428780784, 0.0)\n",
      "Epoch 02 | Loss: 0.6521 | Metrics: (0.7309933159380919, 0.4610169491525424)\n",
      "Epoch 03 | Loss: 0.5979 | Metrics: (0.7761554133752726, 0.5623003194888179)\n",
      "Epoch 04 | Loss: 0.5551 | Metrics: (0.781338242127462, 0.6228571428571429)\n",
      "Epoch 05 | Loss: 0.5189 | Metrics: (0.8007470422132466, 0.5871559633027523)\n",
      "Epoch 06 | Loss: 0.4952 | Metrics: (0.8127032919898488, 0.6462395543175486)\n",
      "Epoch 07 | Loss: 0.4629 | Metrics: (0.8198520213032134, 0.7096774193548387)\n",
      "Epoch 08 | Loss: 0.4404 | Metrics: (0.8282517782464166, 0.6878306878306879)\n",
      "Epoch 09 | Loss: 0.4155 | Metrics: (0.8264467240947921, 0.711779448621554)\n",
      "Epoch 10 | Loss: 0.4138 | Metrics: (0.8290023948243199, 0.7205882352941176)\n",
      "\n",
      "--- Fold 8/10 ---\n",
      "Epoch 01 | Loss: 0.6766 | Metrics: (0.7826485628636926, 0.6962305986696231)\n",
      "Epoch 02 | Loss: 0.5715 | Metrics: (0.8253923470287426, 0.7289719626168223)\n",
      "Epoch 03 | Loss: 0.5530 | Metrics: (0.8392876035972493, 0.7425149700598803)\n",
      "Epoch 04 | Loss: 0.4824 | Metrics: (0.8532886616117086, 0.7575757575757576)\n",
      "Epoch 05 | Loss: 0.4456 | Metrics: (0.8692117792276495, 0.7493796526054591)\n",
      "Epoch 06 | Loss: 0.3947 | Metrics: (0.8643801798624582, 0.7763157894736842)\n",
      "Epoch 07 | Loss: 0.3688 | Metrics: (0.8684359019573268, 0.7759036144578313)\n",
      "Epoch 08 | Loss: 0.3412 | Metrics: (0.870499030153412, 0.7531172069825437)\n",
      "Epoch 09 | Loss: 0.3241 | Metrics: (0.8599012519837771, 0.7720930232558139)\n",
      "Epoch 10 | Loss: 0.3025 | Metrics: (0.8602539234702875, 0.7674418604651162)\n",
      "\n",
      "--- Fold 9/10 ---\n",
      "Epoch 01 | Loss: 0.7047 | Metrics: (0.7388677880267679, 0.6098901098901098)\n",
      "Epoch 02 | Loss: 0.5852 | Metrics: (0.7560680050642069, 0.6339066339066338)\n",
      "Epoch 03 | Loss: 0.5251 | Metrics: (0.817236389943932, 0.702020202020202)\n",
      "Epoch 04 | Loss: 0.4762 | Metrics: (0.8328630855489239, 0.6989795918367347)\n",
      "Epoch 05 | Loss: 0.4318 | Metrics: (0.844130945921505, 0.7180851063829788)\n",
      "Epoch 06 | Loss: 0.3972 | Metrics: (0.8388858744800145, 0.7164179104477613)\n",
      "Epoch 07 | Loss: 0.3682 | Metrics: (0.8468077410019895, 0.7169811320754718)\n",
      "Epoch 08 | Loss: 0.3434 | Metrics: (0.8520708988967263, 0.708215297450425)\n",
      "Epoch 09 | Loss: 0.3272 | Metrics: (0.849267498643516, 0.6994535519125683)\n",
      "Epoch 10 | Loss: 0.3104 | Metrics: (0.8509857117019354, 0.7098445595854922)\n",
      "\n",
      "--- Fold 10/10 ---\n",
      "Epoch 01 | Loss: 0.7374 | Metrics: (0.7017641921397381, 0.5317919075144508)\n",
      "Epoch 02 | Loss: 0.5886 | Metrics: (0.7503755458515284, 0.5967741935483871)\n",
      "Epoch 03 | Loss: 0.5406 | Metrics: (0.7765938864628822, 0.6635294117647059)\n",
      "Epoch 04 | Loss: 0.5097 | Metrics: (0.7990218340611355, 0.6649214659685864)\n",
      "Epoch 05 | Loss: 0.4428 | Metrics: (0.8107598253275109, 0.7308533916849015)\n",
      "Epoch 06 | Loss: 0.4065 | Metrics: (0.8262532751091702, 0.7044334975369458)\n",
      "Epoch 07 | Loss: 0.3733 | Metrics: (0.8318427947598254, 0.7378190255220417)\n",
      "Epoch 08 | Loss: 0.3473 | Metrics: (0.8350218340611354, 0.7540229885057472)\n",
      "Epoch 09 | Loss: 0.3283 | Metrics: (0.8371703056768559, 0.7494145199063231)\n",
      "Epoch 10 | Loss: 0.3214 | Metrics: (0.8373449781659389, 0.7464788732394365)\n",
      "\n",
      "Final BBBP CLASSIFICATION Results over 10 folds:\n",
      "AUC: 0.8381 ± 0.0153\n",
      "F1 : 0.7288 ± 0.0183\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_finetuning(\"BBBP\", dataset_path=\"Ant_Lab_anti_malaria_data_2023.csv\")  # Or \"ESOL\", \"Tox21\", \"Lipophilicity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cb964-bb76-453f-867c-a3a72f2ddcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
